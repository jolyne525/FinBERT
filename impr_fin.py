import os
import io
import math
from dataclasses import dataclass
from typing import Tuple, Optional

import numpy as np
import pandas as pd
import yfinance as yf
import streamlit as st
import plotly.graph_objects as go
from plotly.subplots import make_subplots

import torch
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from statsmodels.tsa.stattools import grangercausalitytests


# -----------------------------
# 0) Basic Config / Styling
# -----------------------------
st.set_page_config(page_title="Sentiment Alpha (FinBERT)", page_icon="ğŸ“ˆ", layout="wide")

st.markdown(
    """
<style>
div[data-testid="stMetric"]{
  background:#f6f8fa;
  padding:12px;
  border-radius:14px;
  border:1px solid #e5e7eb;
}
.block-container{padding-top: 1.8rem;}
</style>
""",
    unsafe_allow_html=True,
)

st.title("ğŸ“ˆ FinBERT æƒ…ç»ªå› å­ Â· Leadâ€“Lag åˆ†æ Â· æ‹©æ—¶ç­–ç•¥å›æµ‹")
st.caption("Pipeline: Tokenization â†’ FinBERT Inference â†’ Daily Aggregation â†’ Time Alignment â†’ Granger Causality (multi-lag) â†’ Backtest")


def beautify_fig(fig, title=None, ytitle=None):
    fig.update_layout(
        template="plotly_white",
        hovermode="x unified",
        title=title,
        yaxis_title=ytitle,
        legend=dict(orientation="h", yanchor="top", y=-0.2, xanchor="left", x=0),
        margin=dict(l=20, r=20, t=55, b=20),
    )
    fig.update_xaxes(showgrid=True)
    fig.update_yaxes(showgrid=True)
    return fig


# -----------------------------
# 1) Sidebar Controls
# -----------------------------
with st.sidebar:
    st.header("ğŸ§ª å®éªŒæ§åˆ¶å°")

    # (Optional) China mirror to improve HF downloads
    use_hf_mirror = st.toggle("ä½¿ç”¨ HF å›½å†…é•œåƒï¼ˆå¯é€‰ï¼‰", value=True)
    if use_hf_mirror:
        os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

    uploaded_file = st.file_uploader("1) ä¸Šä¼ æ–°é—»æ•°æ®ï¼ˆCSV / Excelï¼‰", type=["csv", "xlsx", "xls"])

    ticker = st.text_input("2) è‚¡ç¥¨ä»£ç ï¼ˆYahoo Financeï¼‰", value="AAPL").strip().upper()

    max_lag = st.slider("3) Granger æœ€å¤§æ»åé˜¶ï¼ˆ1..Nï¼‰", 1, 12, 5)

    agg_method = st.selectbox("4) æ—¥åº¦èšåˆæ–¹æ³•", ["mean", "median"], index=0)
    align_mode = st.selectbox("5) æ–°é—»æ—¥æœŸå¯¹é½åˆ°äº¤æ˜“æ—¥", ["next_trading_dayï¼ˆå‘¨æœ«/èŠ‚å‡æ—¥ â†’ ä¸‹ä¸€ä¸ªäº¤æ˜“æ—¥ï¼‰", "same_dayï¼ˆä»…ä¿ç•™é‡å æ—¥æœŸï¼‰"], index=0)

    sentiment_threshold = st.slider("6) æƒ…ç»ªé˜ˆå€¼ï¼ˆ>é˜ˆå€¼æŒæœ‰ï¼‰", -0.5, 0.5, 0.0, 0.01)

    include_cost = st.toggle("7) äº¤æ˜“æˆæœ¬ï¼ˆå¯é€‰ï¼‰", value=False)
    cost_bps = st.slider("   å•è¾¹æˆæœ¬ï¼ˆbpsï¼‰", 0, 50, 5, 1) if include_cost else 0

    rf_annual = st.slider("8) æ— é£é™©åˆ©ç‡ï¼ˆå¹´åŒ–ï¼Œç”¨äº Sharpeï¼‰", 0.0, 0.10, 0.02, 0.005)

    finbert_batch = st.slider("9) FinBERT batch sizeï¼ˆè¶Šå¤§è¶Šå¿«ï¼Œè¶Šå¤§è¶Šå å†…å­˜ï¼‰", 8, 64, 32, 8)

    run_btn = st.button("ğŸš€ è¿è¡Œå…¨æµç¨‹", type="primary")


# -----------------------------
# 2) Data Loading / Parsing
# -----------------------------
def _read_uploaded_file(file) -> pd.DataFrame:
    if file is None:
        return pd.DataFrame()

    # read bytes once
    raw = file.read()
    bio = io.BytesIO(raw)

    if file.name.lower().endswith(".csv"):
        df = pd.read_csv(bio)
    else:
        df = pd.read_excel(bio)

    return df


def _standardize_news_columns(df: pd.DataFrame) -> pd.DataFrame:
    """
    Accept common schemas:
      - title/date/stock
      - headline/date/ticker
      - Headline/Date/Ticker
    Output columns: Date, Headline, Ticker (optional)
    """
    if df.empty:
        return df

    cols = {c: c.strip() for c in df.columns}
    df = df.rename(columns=cols)

    lower_map = {c.lower(): c for c in df.columns}

    # headline/title
    headline_col = None
    for k in ["headline", "title", "text", "news", "content"]:
        if k in lower_map:
            headline_col = lower_map[k]
            break

    # date/time
    date_col = None
    for k in ["date", "datetime", "time", "published", "timestamp"]:
        if k in lower_map:
            date_col = lower_map[k]
            break

    # ticker/stock
    ticker_col = None
    for k in ["ticker", "stock", "symbol"]:
        if k in lower_map:
            ticker_col = lower_map[k]
            break

    if headline_col is None or date_col is None:
        raise ValueError("æ–°é—»æ•°æ®å¿…é¡»åŒ…å«æ—¥æœŸåˆ—ï¼ˆdateï¼‰å’Œæ ‡é¢˜åˆ—ï¼ˆheadline/titleï¼‰ã€‚è¯·æ£€æŸ¥ä½ çš„ CSV/Excel åˆ—åã€‚")

    out = pd.DataFrame()
    out["Headline"] = df[headline_col].astype(str)
    out["Date"] = pd.to_datetime(df[date_col], errors="coerce")

    if ticker_col is not None:
        out["Ticker"] = df[ticker_col].astype(str).str.upper().str.strip()
    else:
        out["Ticker"] = np.nan

    out = out.dropna(subset=["Date", "Headline"])
    out["Headline"] = out["Headline"].replace("nan", np.nan).dropna()
    out["Date"] = pd.to_datetime(out["Date"]).dt.tz_localize(None)
    out["NewsDate"] = out["Date"].dt.normalize()  # midnight
    return out[["NewsDate", "Headline", "Ticker"]]


@st.cache_data(show_spinner=False)
def load_news_cached(file_bytes: bytes, filename: str) -> pd.DataFrame:
    bio = io.BytesIO(file_bytes)
    if filename.lower().endswith(".csv"):
        df = pd.read_csv(bio)
    else:
        df = pd.read_excel(bio)
    return df


@st.cache_data(show_spinner=False)
def get_market_data(ticker: str, start: pd.Timestamp, end: pd.Timestamp) -> pd.DataFrame:
    """
    Download daily market data and compute returns (simple + log).
    Returns: Date, Close, Return, Log_Return
    """
    df = yf.download(ticker, start=start.date(), end=(end + pd.Timedelta(days=1)).date(), progress=False)

    if df is None or df.empty:
        return pd.DataFrame()

    df = df.reset_index()
    # MultiIndex handling
    if isinstance(df.columns, pd.MultiIndex):
        df.columns = [c[0] for c in df.columns]

    col = "Adj Close" if "Adj Close" in df.columns else "Close"
    close = df[col]
    if isinstance(close, pd.DataFrame):
        close = close.iloc[:, 0]
    close = pd.to_numeric(close, errors="coerce")

    out = pd.DataFrame({"Date": pd.to_datetime(df["Date"]).dt.normalize(), "Close": close})
    out = out.dropna().sort_values("Date").reset_index(drop=True)

    out["Return"] = out["Close"].pct_change()
    out["Log_Return"] = np.log(out["Close"] / out["Close"].shift(1))
    out = out.dropna().reset_index(drop=True)
    return out


# -----------------------------
# 3) FinBERT: Load Model + Batch Inference
# -----------------------------
@st.cache_resource(show_spinner=False)
def load_finbert() -> Tuple[AutoTokenizer, AutoModelForSequenceClassification, int, int]:
    tokenizer = AutoTokenizer.from_pretrained("ProsusAI/finbert")
    model = AutoModelForSequenceClassification.from_pretrained("ProsusAI/finbert")
    model.eval()

    # Robustly map label indices
    id2label = {int(k): v for k, v in model.config.id2label.items()} if hasattr(model.config, "id2label") else {}
    label2id = {v.lower(): k for k, v in id2label.items()}

    def find_idx(target: str) -> int:
        # Try exact label
        for k, v in id2label.items():
            if target in v.lower():
                return k
        # fallback typical order [positive, negative, neutral] for ProsusAI/finbert
        if target == "positive":
            return 0
        if target == "negative":
            return 1
        return 2

    pos_idx = find_idx("positive")
    neg_idx = find_idx("negative")
    return tokenizer, model, pos_idx, neg_idx


def finbert_infer_scores(
    texts: list,
    tokenizer,
    model,
    pos_idx: int,
    neg_idx: int,
    batch_size: int = 32,
) -> np.ndarray:
    device = torch.device("cpu")
    model.to(device)

    scores = []
    n = len(texts)
    for i in range(0, n, batch_size):
        batch = texts[i : i + batch_size]
        enc = tokenizer(
            batch,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=128,
        )
        enc = {k: v.to(device) for k, v in enc.items()}
        with torch.no_grad():
            logits = model(**enc).logits
            probs = F.softmax(logits, dim=-1).cpu().numpy()

        s = probs[:, pos_idx] - probs[:, neg_idx]  # continuous factor in [-1, 1]
        scores.extend(s.tolist())
    return np.array(scores, dtype=np.float32)


# -----------------------------
# 4) Alignment: News â†’ Trading Day
# -----------------------------
def build_daily_sentiment(news_df: pd.DataFrame, method: str = "mean") -> pd.DataFrame:
    """
    Aggregate headline-level sentiment to daily sentiment factor.
    """
    if news_df.empty:
        return pd.DataFrame()
    g = news_df.groupby("NewsDate")["Sentiment"]
    daily = g.mean() if method == "mean" else g.median()
    out = daily.reset_index().rename(columns={"NewsDate": "Date", "Sentiment": "Sentiment_Factor"})
    out["Date"] = pd.to_datetime(out["Date"]).dt.normalize()
    return out.sort_values("Date").reset_index(drop=True)


def align_sentiment_to_market(
    daily_sent: pd.DataFrame,
    market: pd.DataFrame,
    mode: str,
) -> pd.DataFrame:
    """
    Align sentiment factor to market trading days.
    mode:
      - next_trading_day: map sentiment date -> next available market date (forward asof)
      - same_day: inner join on exact date
    """
    if daily_sent.empty or market.empty:
        return pd.DataFrame()

    mkt = market[["Date"]].sort_values("Date").reset_index(drop=True)
    sent = daily_sent.copy().sort_values("Date").reset_index(drop=True)

    if mode.startswith("next_trading_day"):
        sent = sent.rename(columns={"Date": "NewsDate"})
        aligned = pd.merge_asof(
            sent.sort_values("NewsDate"),
            mkt.sort_values("Date"),
            left_on="NewsDate",
            right_on="Date",
            direction="forward",
            allow_exact_matches=True,
        )
        aligned = aligned.rename(columns={"Date": "TradeDate"})
        aligned = aligned.dropna(subset=["TradeDate"])
        # multiple news days may map to same trade day => re-aggregate
        aligned = (
            aligned.groupby("TradeDate")["Sentiment_Factor"]
            .mean()
            .reset_index()
            .rename(columns={"TradeDate": "Date"})
        )
        aligned["Date"] = pd.to_datetime(aligned["Date"]).dt.normalize()
        return aligned.sort_values("Date").reset_index(drop=True)

    # same_day
    aligned = pd.merge(daily_sent, market[["Date"]], on="Date", how="inner")
    return aligned[["Date", "Sentiment_Factor"]].sort_values("Date").reset_index(drop=True)


# -----------------------------
# 5) Granger: multi-lag + both directions
# -----------------------------
@dataclass
class GrangerResultRow:
    lag: int
    p_sent_to_ret: float
    p_ret_to_sent: float


def run_granger_multi_lag(merged: pd.DataFrame, max_lag: int) -> pd.DataFrame:
    """
    For statsmodels.grangercausalitytests:
      Passing array with columns [y, x] tests whether x Granger-causes y.
    We test:
      - Sentiment -> Return: y=Return, x=Sentiment
      - Return -> Sentiment: y=Sentiment, x=Return
    """
    df = merged[["Return", "Sentiment_Factor"]].dropna().copy()
    df = df.sort_values("Date").reset_index(drop=True)

    # Need enough samples: roughly > max_lag + 10 to be safe
    if len(df) < (max_lag + 12):
        raise ValueError(f"æ ·æœ¬å¤ªå°‘ï¼šéœ€è¦è‡³å°‘ ~{max_lag+12} è¡Œï¼Œå½“å‰åªæœ‰ {len(df)} è¡Œã€‚è¯·æ‰©å¤§æ—¥æœŸèŒƒå›´æˆ–æä¾›æ›´å¤šæ–°é—»ã€‚")

    # Sentiment -> Return
    ts_sr = df[["Return", "Sentiment_Factor"]].to_numpy()
    res_sr = grangercausalitytests(ts_sr, maxlag=max_lag, verbose=False)

    # Return -> Sentiment
    ts_rs = df[["Sentiment_Factor", "Return"]].to_numpy()
    res_rs = grangercausalitytests(ts_rs, maxlag=max_lag, verbose=False)

    rows = []
    for lag in range(1, max_lag + 1):
        # choose a common test: ssr_ftest p-value
        p1 = float(res_sr[lag][0]["ssr_ftest"][1])
        p2 = float(res_rs[lag][0]["ssr_ftest"][1])
        rows.append(GrangerResultRow(lag=lag, p_sent_to_ret=p1, p_ret_to_sent=p2))

    out = pd.DataFrame([r.__dict__ for r in rows])
    out = out.rename(
        columns={
            "lag": "Lag",
            "p_sent_to_ret": "P-value (Sentiment â†’ Return)",
            "p_ret_to_sent": "P-value (Return â†’ Sentiment)",
        }
    )
    return out


# -----------------------------
# 6) Backtest: timing policy + risk metrics
# -----------------------------
def max_drawdown(equity: pd.Series) -> float:
    peak = equity.cummax()
    dd = equity / peak - 1.0
    return float(dd.min())  # negative number


def sharpe_ratio(daily_ret: pd.Series, rf_annual: float = 0.02) -> float:
    rf_daily = rf_annual / 252.0
    excess = daily_ret - rf_daily
    vol = excess.std()
    if vol == 0 or np.isnan(vol):
        return 0.0
    return float(excess.mean() / vol * math.sqrt(252))


def run_timing_backtest(
    merged: pd.DataFrame,
    threshold: float = 0.0,
    cost_bps: int = 0,
    rf_annual: float = 0.02,
) -> Tuple[pd.DataFrame, dict]:
    """
    Policy:
      position[t] = 1 if sentiment[t-1] > threshold else 0
      strategy_ret[t] = position[t] * market_ret[t] - cost * |position[t]-position[t-1]|
    """
    df = merged[["Date", "Close", "Return", "Sentiment_Factor"]].dropna().copy()
    df = df.sort_values("Date").reset_index(drop=True)

    df["Position"] = (df["Sentiment_Factor"].shift(1) > threshold).astype(int)
    df["Position"] = df["Position"].fillna(0).astype(int)

    # turnover: trade when position changes
    df["Trade"] = df["Position"].diff().abs().fillna(0)
    tc = (cost_bps / 10000.0) * df["Trade"]  # proportion cost on trade days

    df["Strategy_Return"] = df["Position"] * df["Return"] - tc
    df["Benchmark_Return"] = df["Return"]

    # equity curves
    df["Equity_Strategy"] = (1.0 + df["Strategy_Return"]).cumprod()
    df["Equity_Benchmark"] = (1.0 + df["Benchmark_Return"]).cumprod()

    # drawdown series
    df["DD_Strategy"] = df["Equity_Strategy"] / df["Equity_Strategy"].cummax() - 1.0
    df["DD_Benchmark"] = df["Equity_Benchmark"] / df["Equity_Benchmark"].cummax() - 1.0

    # metrics
    strat_cum = float(df["Equity_Strategy"].iloc[-1] - 1.0)
    bench_cum = float(df["Equity_Benchmark"].iloc[-1] - 1.0)

    strat_sharpe = sharpe_ratio(df["Strategy_Return"], rf_annual=rf_annual)
    bench_sharpe = sharpe_ratio(df["Benchmark_Return"], rf_annual=rf_annual)

    strat_mdd = max_drawdown(df["Equity_Strategy"])
    bench_mdd = max_drawdown(df["Equity_Benchmark"])

    strat_vol = float(df["Strategy_Return"].std() * math.sqrt(252))
    bench_vol = float(df["Benchmark_Return"].std() * math.sqrt(252))

    n_trades = int(df["Trade"].sum())
    exposure = float(df["Position"].mean())

    metrics = {
        "Strategy Cumulative Return": strat_cum,
        "Benchmark Cumulative Return": bench_cum,
        "Alpha (Strategy - Benchmark)": strat_cum - bench_cum,
        "Strategy Sharpe": strat_sharpe,
        "Benchmark Sharpe": bench_sharpe,
        "Strategy Max Drawdown": strat_mdd,
        "Benchmark Max Drawdown": bench_mdd,
        "Strategy Vol (ann.)": strat_vol,
        "Benchmark Vol (ann.)": bench_vol,
        "Trades": n_trades,
        "Exposure": exposure,
        "Transaction Cost (bps)": cost_bps,
        "Sentiment Threshold": threshold,
    }
    return df, metrics


# -----------------------------
# 7) Main Run
# -----------------------------
tabs = st.tabs(["â‘  æ•°æ®ä¸ç®¡çº¿", "â‘¡ æƒ…ç»ªå› å­", "â‘¢ Leadâ€“Lagï¼ˆGrangerï¼‰", "â‘£ ç­–ç•¥å›æµ‹", "â‘¤ å¯¼å‡º"])


if not run_btn:
    with tabs[0]:
        st.info("ğŸ‘ˆ å…ˆåœ¨å·¦ä¾§ä¸Šä¼ æ–°é—»æ•°æ®å¹¶è®¾ç½®å‚æ•°ï¼Œç„¶åç‚¹å‡» **è¿è¡Œå…¨æµç¨‹**ã€‚")
        st.markdown(
            """
**ä½ è¿™ä»½ App å°†å±•ç¤ºï¼š**
- æ–°é—»æ ‡é¢˜ â†’ FinBERT â†’ è¿ç»­æƒ…ç»ªå› å­ï¼ˆæ—¥åº¦èšåˆï¼‰
- æƒ…ç»ªå› å­ä¸æ”¶ç›Šåºåˆ—å¯¹é½ï¼ˆæ”¯æŒå‘¨æœ«æ–°é—»æ˜ å°„åˆ°ä¸‹ä¸€äº¤æ˜“æ—¥ï¼‰
- Granger å› æœæ£€éªŒï¼ˆ1..N é˜¶ï¼Œè¾“å‡º p-valuesï¼‰
- æ‹©æ—¶ç­–ç•¥å›æµ‹ï¼ˆæ˜¨æ—¥æƒ…ç»ª>é˜ˆå€¼æŒæœ‰ï¼Œå¦åˆ™ç©ºä»“ï¼‰+ é£é™©è°ƒæ•´æŒ‡æ ‡
"""
        )
    st.stop()


if uploaded_file is None:
    st.error("è¯·å…ˆä¸Šä¼ æ–°é—» CSV/Excel æ–‡ä»¶ã€‚")
    st.stop()

# Read file bytes for caching
file_bytes = uploaded_file.getvalue()
raw_df = load_news_cached(file_bytes, uploaded_file.name)

try:
    news = _standardize_news_columns(raw_df)
except Exception as e:
    st.error(str(e))
    st.stop()

# Filter by ticker if ticker column exists with values
if news["Ticker"].notna().any():
    if ticker in set(news["Ticker"].dropna().unique()):
        news = news[news["Ticker"] == ticker].copy()
    else:
        st.warning(f"æ–°é—»æ–‡ä»¶ä¸­æœªå‘ç° Ticker={ticker} çš„è®°å½•ï¼Œå°†å¯¹å…¨éƒ¨æ–°é—»åšæƒ…ç»ªè®¡ç®—ï¼ˆä½ ä¹Ÿå¯ä»¥æ¢ä¸ª ticker æˆ–æ£€æŸ¥æ–‡ä»¶ï¼‰ã€‚")

if news.empty:
    st.error("æ¸…æ´—/ç­›é€‰åæ–°é—»ä¸ºç©ºã€‚è¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶ã€‚")
    st.stop()

# Determine date range from news
min_news_date = pd.to_datetime(news["NewsDate"].min()).normalize()
max_news_date = pd.to_datetime(news["NewsDate"].max()).normalize()
start = min_news_date - pd.Timedelta(days=5)
end = max_news_date + pd.Timedelta(days=10)

# Market data
with st.spinner(f"ä¸‹è½½ {ticker} å¸‚åœºæ•°æ®å¹¶è®¡ç®—æ”¶ç›Šåºåˆ—..."):
    market = get_market_data(ticker, start=start, end=end)

if market.empty:
    st.error("æ— æ³•è·å–å¸‚åœºæ•°æ®ï¼ˆYahoo Financeï¼‰ã€‚è¯·æ£€æŸ¥ ticker æˆ–ç½‘ç»œã€‚")
    st.stop()

# FinBERT
with st.spinner("åŠ è½½ FinBERT æ¨¡å‹..."):
    tokenizer, finbert_model, pos_idx, neg_idx = load_finbert()

# Sentiment inference (batched)
with st.spinner("FinBERT æ¨ç†ï¼šå°†æ–°é—»æ ‡é¢˜è½¬æ¢ä¸ºè¿ç»­æƒ…ç»ªå› å­..."):
    headlines = news["Headline"].astype(str).tolist()
    prog = st.progress(0.0)
    scores = []

    # batch loop with progress
    n = len(headlines)
    step = max(finbert_batch, 1)
    for i in range(0, n, step):
        batch = headlines[i : i + step]
        batch_scores = finbert_infer_scores(batch, tokenizer, finbert_model, pos_idx, neg_idx, batch_size=len(batch))
        scores.extend(batch_scores.tolist())
        prog.progress(min(1.0, (i + step) / n))
    prog.empty()

news = news.reset_index(drop=True)
news["Sentiment"] = np.array(scores, dtype=np.float32)

# Daily aggregate (NLP â†’ factor)
daily_sent = build_daily_sentiment(news, method=agg_method)

# Align to market trading day (time alignment)
aligned_sent = align_sentiment_to_market(
    daily_sent,
    market,
    mode=align_mode,
)

# Merge aligned sentiment with market returns
merged = pd.merge(market, aligned_sent, on="Date", how="inner").sort_values("Date").reset_index(drop=True)

if len(merged) < 30:
    st.warning(f"åˆå¹¶åçš„æœ‰æ•ˆæ ·æœ¬è¾ƒå°‘ï¼ˆ{len(merged)} è¡Œï¼‰ã€‚å¯èƒ½å¯¼è‡´ Granger æ£€éªŒä¸ç¨³å®šã€‚å»ºè®®æ‰©å¤§æ–°é—»æ—¥æœŸè¦†ç›–æˆ–æ¢æ›´é•¿æ—¶é—´çª—å£ã€‚")

# -----------------------------
# Tab â‘ : Data & Pipeline
# -----------------------------
with tabs[0]:
    st.subheader("â‘  æ•°æ®ä¸ç®¡çº¿æ¦‚è§ˆï¼ˆNLP â†’ å› å­ â†’ å¯¹é½ï¼‰")

    c1, c2, c3 = st.columns(3)
    c1.metric("æ–°é—»æ¡æ•°ï¼ˆheadline-levelï¼‰", f"{len(news)}")
    c2.metric("æƒ…ç»ªæ—¥åº¦ç‚¹æ•°ï¼ˆdaily factorï¼‰", f"{len(daily_sent)}")
    c3.metric("åˆå¹¶åæ ·æœ¬ï¼ˆå¯¹é½åˆ°äº¤æ˜“æ—¥ï¼‰", f"{len(merged)}")

    left, right = st.columns(2)

    with left:
        st.markdown("**æ–°é—»æ•°æ®ï¼ˆæ¸…æ´—åï¼‰**")
        st.dataframe(news[["NewsDate", "Headline", "Sentiment"]].head(10), use_container_width=True, height=280)

    with right:
        st.markdown("**å¸‚åœºæ•°æ®ï¼ˆæ”¶ç›Šåºåˆ—ï¼‰**")
        st.dataframe(market.head(10), use_container_width=True, height=280)

    st.markdown("**å¯¹é½åçš„æ•°æ®ï¼ˆç”¨äºå› æœæ£€éªŒä¸å›æµ‹ï¼‰**")
    st.dataframe(merged.head(15), use_container_width=True, height=240)

# -----------------------------
# Tab â‘¡: Sentiment Factor Visualization
# -----------------------------
with tabs[1]:
    st.subheader("â‘¡ æƒ…ç»ªå› å­ï¼ˆFinBERTï¼‰ä¸ä»·æ ¼èµ°åŠ¿")

    # Factor distribution
    fig_hist = go.Figure()
    fig_hist.add_trace(go.Histogram(x=news["Sentiment"], nbinsx=50, name="Headline Sentiment"))
    beautify_fig(fig_hist, title="Headline-level Sentiment Distribution", ytitle="Count")
    st.plotly_chart(fig_hist, use_container_width=True)

    # Price + sentiment factor subplot
    fig = make_subplots(specs=[[{"secondary_y": True}]])
    fig.add_trace(
        go.Scatter(x=merged["Date"], y=merged["Close"], name=f"{ticker} Close", mode="lines", line=dict(width=2)),
        secondary_y=False,
    )
    fig.add_trace(
        go.Bar(x=merged["Date"], y=merged["Sentiment_Factor"], name="Daily Sentiment Factor", opacity=0.55),
        secondary_y=True,
    )
    fig.update_yaxes(title_text="Price", secondary_y=False)
    fig.update_yaxes(title_text="Sentiment Factor", secondary_y=True)
    beautify_fig(fig, title="Aligned Sentiment Factor vs Price")
    st.plotly_chart(fig, use_container_width=True)

# -----------------------------
# Tab â‘¢: Leadâ€“Lag (Granger) across multiple lags
# -----------------------------
with tabs[2]:
    st.subheader("â‘¢ Leadâ€“Lag ç»“æ„æ£€éªŒï¼šGranger Causalityï¼ˆå¤šæ»åé˜¶ï¼‰")
    st.caption("åŒæ—¶æŠ¥å‘Š Sentimentâ†’Return ä¸ Returnâ†’Sentiment çš„ p-valuesï¼ˆ1..MaxLagï¼‰ã€‚")

    try:
        gr_df = run_granger_multi_lag(merged, max_lag=max_lag)
        st.dataframe(gr_df.style.format({c: "{:.4f}" for c in gr_df.columns if "P-value" in c}), use_container_width=True)

        # Plot p-values across lags
        fig_p = go.Figure()
        fig_p.add_trace(go.Scatter(x=gr_df["Lag"], y=gr_df["P-value (Sentiment â†’ Return)"], mode="lines+markers", name="Sentiment â†’ Return"))
        fig_p.add_trace(go.Scatter(x=gr_df["Lag"], y=gr_df["P-value (Return â†’ Sentiment)"], mode="lines+markers", name="Return â†’ Sentiment"))
        fig_p.add_hline(y=0.05, line_dash="dash", annotation_text="0.05", annotation_position="top left")
        beautify_fig(fig_p, title="Granger p-values across lag orders", ytitle="p-value")
        st.plotly_chart(fig_p, use_container_width=True)

        sig_sr = (gr_df["P-value (Sentiment â†’ Return)"] < 0.05).any()
        best_lag = int(gr_df.loc[gr_df["P-value (Sentiment â†’ Return)"].idxmin(), "Lag"])
        best_p = float(gr_df["P-value (Sentiment â†’ Return)"].min())

        if sig_sr:
            st.success(f"âœ… æ£€æµ‹åˆ° **Sentiment â†’ Return** åœ¨æŸäº›æ»åé˜¶ä¸Šæ˜¾è‘—ï¼ˆp<0.05ï¼‰ã€‚æœ€å° p-value å‡ºç°åœ¨ lag={best_lag}ï¼ˆp={best_p:.4f}ï¼‰ã€‚")
        else:
            st.info(f"å½“å‰æ ·æœ¬ä¸‹æœªæ£€æµ‹åˆ°æ˜¾è‘—çš„ Sentiment â†’ Returnï¼ˆp<0.05ï¼‰ã€‚æœ€å° p-valueï¼šlag={best_lag}ï¼ˆp={best_p:.4f}ï¼‰ã€‚")

    except Exception as e:
        st.warning(f"Granger æ£€éªŒæ— æ³•æ‰§è¡Œï¼š{e}")

# -----------------------------
# Tab â‘£: Backtest Timing Policy + Risk-Adjusted Metrics
# -----------------------------
with tabs[3]:
    st.subheader("â‘£ æ‹©æ—¶ç­–ç•¥å›æµ‹ï¼ˆTiming Policyï¼‰")
    st.markdown("ç­–ç•¥è§„åˆ™ï¼š**æ˜¨æ—¥æƒ…ç»ªå› å­ > é˜ˆå€¼ â†’ ä»Šæ—¥æŒæœ‰ï¼›å¦åˆ™ç©ºä»“**ã€‚")

    bt_df, metrics = run_timing_backtest(
        merged,
        threshold=sentiment_threshold,
        cost_bps=cost_bps,
        rf_annual=rf_annual,
    )

    # Metrics cards
    c1, c2, c3 = st.columns(3)
    c1.metric("ç­–ç•¥ç´¯è®¡æ”¶ç›Š", f"{metrics['Strategy Cumulative Return']*100:.2f}%", delta=f"vs åŸºå‡† {metrics['Benchmark Cumulative Return']*100:.2f}%")
    c2.metric("ç­–ç•¥ Sharpe", f"{metrics['Strategy Sharpe']:.2f}", delta=f"vs åŸºå‡† {metrics['Benchmark Sharpe']:.2f}")
    c3.metric("ç­–ç•¥æœ€å¤§å›æ’¤", f"{metrics['Strategy Max Drawdown']*100:.2f}%", delta=f"vs åŸºå‡† {metrics['Benchmark Max Drawdown']*100:.2f}%")

    c4, c5, c6 = st.columns(3)
    c4.metric("Alphaï¼ˆç­–ç•¥-åŸºå‡†ï¼‰", f"{metrics['Alpha (Strategy - Benchmark)']*100:.2f}%")
    c5.metric("äº¤æ˜“æ¬¡æ•°ï¼ˆæ¢ä»“ï¼‰", f"{metrics['Trades']}")
    c6.metric("æš´éœ²åº¦ï¼ˆæŒä»“æ¯”ä¾‹ï¼‰", f"{metrics['Exposure']*100:.1f}%")

    # Equity curve plot
    fig_eq = go.Figure()
    fig_eq.add_trace(go.Scatter(x=bt_df["Date"], y=bt_df["Equity_Strategy"], name="Strategy Equity", mode="lines", line=dict(width=3)))
    fig_eq.add_trace(go.Scatter(x=bt_df["Date"], y=bt_df["Equity_Benchmark"], name="Buy&Hold Equity", mode="lines", line=dict(dash="dash")))
    beautify_fig(fig_eq, title="Equity Curve: Strategy vs Benchmark", ytitle="Equity")
    st.plotly_chart(fig_eq, use_container_width=True)

    # Drawdown plot
    fig_dd = go.Figure()
    fig_dd.add_trace(go.Scatter(x=bt_df["Date"], y=bt_df["DD_Strategy"], name="Strategy Drawdown", mode="lines"))
    fig_dd.add_trace(go.Scatter(x=bt_df["Date"], y=bt_df["DD_Benchmark"], name="Benchmark Drawdown", mode="lines", line=dict(dash="dash")))
    beautify_fig(fig_dd, title="Drawdown: Strategy vs Benchmark", ytitle="Drawdown")
    st.plotly_chart(fig_dd, use_container_width=True)

    # Signal & Sentiment visualization
    fig_sig = make_subplots(specs=[[{"secondary_y": True}]])
    fig_sig.add_trace(go.Scatter(x=bt_df["Date"], y=bt_df["Sentiment_Factor"], name="Sentiment Factor", mode="lines"), secondary_y=True)
    fig_sig.add_trace(go.Scatter(x=bt_df["Date"], y=bt_df["Position"], name="Position (0/1)", mode="lines", line=dict(width=2)), secondary_y=False)
    fig_sig.update_yaxes(title_text="Position", secondary_y=False)
    fig_sig.update_yaxes(title_text="Sentiment", secondary_y=True)
    beautify_fig(fig_sig, title="Signal Construction: Prior-day Sentiment â†’ Position")
    st.plotly_chart(fig_sig, use_container_width=True)

    st.markdown("**å›æµ‹æ˜ç»†ï¼ˆå‰ 30 è¡Œï¼‰**")
    st.dataframe(bt_df.head(30), use_container_width=True, height=260)

# -----------------------------
# Tab â‘¤: Export / Download
# -----------------------------
with tabs[4]:
    st.subheader("â‘¤ å¯¼å‡ºï¼ˆå¯å¤ç°å®éªŒï¼‰")
    st.caption("ä¸‹è½½å¯¹é½åçš„æ•°æ®ã€Granger ç»“æœã€å›æµ‹æ˜ç»†ï¼Œæ–¹ä¾¿ä½ åœ¨ notebook / æŠ¥å‘Šä¸­å¤ç°ä¸ç»˜å›¾ã€‚")

    # merged data download
    merged_csv = merged.to_csv(index=False).encode("utf-8")
    st.download_button("â¬‡ï¸ ä¸‹è½½å¯¹é½åçš„æ•°æ®ï¼ˆnews-factor-market alignedï¼‰", merged_csv, file_name=f"{ticker}_aligned_data.csv", mime="text/csv")

    # granger result download
    try:
        gr_df = run_granger_multi_lag(merged, max_lag=max_lag)
        gr_csv = gr_df.to_csv(index=False).encode("utf-8")
        st.download_button("â¬‡ï¸ ä¸‹è½½ Granger ç»“æœï¼ˆmulti-lag p-valuesï¼‰", gr_csv, file_name=f"{ticker}_granger_pvalues.csv", mime="text/csv")
    except Exception:
        st.info("Granger ç»“æœä¸å¯ç”¨ï¼ˆæ ·æœ¬ä¸è¶³æˆ–æ£€éªŒå¤±è´¥ï¼‰ã€‚")

    # backtest detail download
    bt_df, metrics = run_timing_backtest(
        merged,
        threshold=sentiment_threshold,
        cost_bps=cost_bps,
        rf_annual=rf_annual,
    )
    bt_csv = bt_df.to_csv(index=False).encode("utf-8")
    st.download_button("â¬‡ï¸ ä¸‹è½½å›æµ‹æ˜ç»†ï¼ˆpositions/returns/equityï¼‰", bt_csv, file_name=f"{ticker}_backtest_detail.csv", mime="text/csv")

    # metrics download
    metrics_df = pd.DataFrame([metrics])
    metrics_csv = metrics_df.to_csv(index=False).encode("utf-8")
    st.download_button("â¬‡ï¸ ä¸‹è½½æŒ‡æ ‡æ±‡æ€»ï¼ˆmetricsï¼‰", metrics_csv, file_name=f"{ticker}_metrics.csv", mime="text/csv")
