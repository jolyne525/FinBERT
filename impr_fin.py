import os
import io
import math
import time
import uuid
import queue
import threading
from dataclasses import dataclass
from typing import Tuple, Optional, List, Dict

import numpy as np
import pandas as pd
import yfinance as yf
import streamlit as st
import plotly.graph_objects as go
from plotly.subplots import make_subplots

import torch
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from statsmodels.tsa.stattools import grangercausalitytests


# -----------------------------
# 0) Basic Config / Styling
# -----------------------------
st.set_page_config(page_title="Sentiment Alpha (FinBERT)", page_icon="ğŸ“ˆ", layout="wide")

st.markdown(
    """
<style>
div[data-testid="stMetric"]{
  background:#f6f8fa;
  padding:12px;
  border-radius:14px;
  border:1px solid #e5e7eb;
}
.block-container{padding-top: 1.8rem;}
</style>
""",
    unsafe_allow_html=True,
)

st.title("ğŸ“ˆ FinBERT æƒ…ç»ªå› å­ Â· Leadâ€“Lag åˆ†æ Â· æ‹©æ—¶ç­–ç•¥å›æµ‹ï¼ˆé•¿æ–‡æœ¬ + å¼‚æ­¥æ¨ç†ï¼‰")
st.caption("Pipeline: Tokenization â†’ FinBERT (sliding window/pooling) â†’ Daily Aggregation â†’ Time Alignment â†’ Granger (multi-lag) â†’ Backtest")


def beautify_fig(fig, title=None, ytitle=None):
    fig.update_layout(
        template="plotly_white",
        hovermode="x unified",
        title=title,
        yaxis_title=ytitle,
        legend=dict(orientation="h", yanchor="top", y=-0.2, xanchor="left", x=0),
        margin=dict(l=20, r=20, t=55, b=20),
    )
    fig.update_xaxes(showgrid=True)
    fig.update_yaxes(showgrid=True)
    return fig


def _rerun():
    # å…¼å®¹ä¸åŒ streamlit ç‰ˆæœ¬
    if hasattr(st, "rerun"):
        st.rerun()
    else:
        st.experimental_rerun()


# -----------------------------
# 1) Sidebar Controls
# -----------------------------
with st.sidebar:
    st.header("ğŸ§ª å®éªŒæ§åˆ¶å°")

    # (Optional) China mirror to improve HF downloads
    use_hf_mirror = st.toggle("ä½¿ç”¨ HF å›½å†…é•œåƒï¼ˆå¯é€‰ï¼‰", value=True)
    if use_hf_mirror:
        os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

    uploaded_file = st.file_uploader("1) ä¸Šä¼ æ–°é—»æ•°æ®ï¼ˆCSV / Excelï¼‰", type=["csv", "xlsx", "xls"])

    ticker = st.text_input("2) è‚¡ç¥¨ä»£ç ï¼ˆYahoo Financeï¼‰", value="AAPL").strip().upper()

    max_lag = st.slider("3) Granger æœ€å¤§æ»åé˜¶ï¼ˆ1..Nï¼‰", 1, 12, 5)

    agg_method = st.selectbox("4) æ—¥åº¦èšåˆæ–¹æ³•", ["mean", "median"], index=0)
    align_mode = st.selectbox(
        "5) æ–°é—»æ—¥æœŸå¯¹é½åˆ°äº¤æ˜“æ—¥",
        ["next_trading_dayï¼ˆå‘¨æœ«/èŠ‚å‡æ—¥ â†’ ä¸‹ä¸€ä¸ªäº¤æ˜“æ—¥ï¼‰", "same_dayï¼ˆä»…ä¿ç•™é‡å æ—¥æœŸï¼‰"],
        index=0,
    )

    sentiment_threshold = st.slider("6) æƒ…ç»ªé˜ˆå€¼ï¼ˆ>é˜ˆå€¼æŒæœ‰ï¼‰", -0.5, 0.5, 0.0, 0.01)

    include_cost = st.toggle("7) äº¤æ˜“æˆæœ¬ï¼ˆå¯é€‰ï¼‰", value=False)
    cost_bps = st.slider("   å•è¾¹æˆæœ¬ï¼ˆbpsï¼‰", 0, 50, 5, 1) if include_cost else 0

    rf_annual = st.slider("8) æ— é£é™©åˆ©ç‡ï¼ˆå¹´åŒ–ï¼Œç”¨äº Sharpeï¼‰", 0.0, 0.10, 0.02, 0.005)

    # ---------- é•¿æ–‡æœ¬ï¼ˆsliding windowï¼‰æ§åˆ¶ ----------
    st.markdown("---")
    st.subheader("ğŸ§© é•¿æ–‡æœ¬æ”¯æŒï¼ˆsliding windowï¼‰")

    longtext_mode = st.selectbox(
        "æ–‡æœ¬é•¿åº¦å¤„ç†æ¨¡å¼",
        ["autoï¼ˆè¶…è¿‡512 tokensæ‰åˆ‡åˆ†ï¼‰", "alwaysï¼ˆå¼ºåˆ¶åˆ‡åˆ†ï¼‰", "offï¼ˆä¸åˆ‡åˆ†/ç›´æ¥æˆªæ–­ï¼‰"],
        index=0,
        help="autoï¼šæ ‡é¢˜çŸ­æ–‡æœ¬ä¸ä¼šåˆ‡åˆ†ï¼›é•¿æ–‡æ¡£ä¼šæŒ‰çª—å£åˆ‡åˆ†å¹¶èšåˆã€‚alwaysï¼šæ‰€æœ‰æ–‡æœ¬éƒ½åˆ‡åˆ†ã€‚",
    )
    window_max_len = st.selectbox("çª—å£ max_length", [128, 256, 512], index=2, help="BERT/FinBERT æœ€å¤§é€šå¸¸ä¸º 512 tokensã€‚é•¿æ–‡æœ¬è¯·ç”¨ 512ã€‚")
    window_stride = st.slider("æ»‘çª— strideï¼ˆé‡å ï¼‰", 0, 256, 128, 16, help="stride è¶Šå¤§é‡å è¶Šå¤šï¼Œè¶Šèƒ½ä¿ç•™è·¨æ®µä¿¡æ¯ï¼Œä½†æ›´è€—æ—¶ã€‚")
    pooling = st.selectbox("çª—å£èšåˆ pooling", ["mean", "max"], index=0, help="meanï¼šæ›´å¹³æ»‘ï¼›maxï¼šæ›´æ•æ„Ÿï¼ˆæŠ“å¼ºæƒ…ç»ªç‰‡æ®µï¼‰ã€‚")

    # ---------- å¼‚æ­¥æ¨ç† ----------
    st.markdown("---")
    st.subheader("âš¡ å¼‚æ­¥æ¨ç†ï¼ˆä¸å¡ UIï¼‰")
    enable_async = st.toggle("å¯ç”¨å¼‚æ­¥æ¨ç†ï¼ˆæ¨èï¼‰", value=True, help="å¯ç”¨åï¼šæ¨ç†åœ¨åå°çº¿ç¨‹è·‘ï¼Œç•Œé¢ä¸ä¼šè¢«å¡ä½ã€‚")

    finbert_batch = st.slider("FinBERT batch sizeï¼ˆè¶Šå¤§è¶Šå¿«ï¼Œè¶Šå¤§è¶Šå å†…å­˜ï¼‰", 8, 128, 32, 8)

    run_btn = st.button("ğŸš€ è¿è¡Œå…¨æµç¨‹", type="primary")


# -----------------------------
# 2) Data Loading / Parsing
# -----------------------------
@st.cache_data(show_spinner=False)
def load_news_cached(file_bytes: bytes, filename: str) -> pd.DataFrame:
    bio = io.BytesIO(file_bytes)
    if filename.lower().endswith(".csv"):
        df = pd.read_csv(bio)
    else:
        df = pd.read_excel(bio)
    return df


def _standardize_news_columns(df: pd.DataFrame) -> pd.DataFrame:
    """
    Accept common schemas:
      - title/date/stock
      - headline/date/ticker
      - Headline/Date/Ticker
    Output columns: NewsDate, Headline, Ticker (optional)
    """
    if df.empty:
        return df

    cols = {c: c.strip() for c in df.columns}
    df = df.rename(columns=cols)
    lower_map = {c.lower(): c for c in df.columns}

    headline_col = None
    for k in ["headline", "title", "text", "news", "content"]:
        if k in lower_map:
            headline_col = lower_map[k]
            break

    date_col = None
    for k in ["date", "datetime", "time", "published", "timestamp"]:
        if k in lower_map:
            date_col = lower_map[k]
            break

    ticker_col = None
    for k in ["ticker", "stock", "symbol"]:
        if k in lower_map:
            ticker_col = lower_map[k]
            break

    if headline_col is None or date_col is None:
        raise ValueError("æ–°é—»æ•°æ®å¿…é¡»åŒ…å«æ—¥æœŸåˆ—ï¼ˆdateï¼‰å’Œæ ‡é¢˜/æ­£æ–‡åˆ—ï¼ˆheadline/title/text/contentï¼‰ã€‚è¯·æ£€æŸ¥ä½ çš„ CSV/Excel åˆ—åã€‚")

    out = pd.DataFrame()
    out["Headline"] = df[headline_col].astype(str)
    out["Date"] = pd.to_datetime(df[date_col], errors="coerce")

    if ticker_col is not None:
        out["Ticker"] = df[ticker_col].astype(str).str.upper().str.strip()
    else:
        out["Ticker"] = np.nan

    out = out.dropna(subset=["Date", "Headline"])
    out["Headline"] = out["Headline"].replace("nan", np.nan).dropna()
    out["Date"] = pd.to_datetime(out["Date"]).dt.tz_localize(None)
    out["NewsDate"] = out["Date"].dt.normalize()  # midnight
    return out[["NewsDate", "Headline", "Ticker"]]


@st.cache_data(show_spinner=False)
def get_market_data(ticker: str, start: pd.Timestamp, end: pd.Timestamp) -> pd.DataFrame:
    """
    Download daily market data and compute returns (simple + log).
    Returns: Date, Close, Return, Log_Return
    """
    df = yf.download(ticker, start=start.date(), end=(end + pd.Timedelta(days=1)).date(), progress=False)
    if df is None or df.empty:
        return pd.DataFrame()

    df = df.reset_index()
    if isinstance(df.columns, pd.MultiIndex):
        df.columns = [c[0] for c in df.columns]

    col = "Adj Close" if "Adj Close" in df.columns else "Close"
    close = df[col]
    if isinstance(close, pd.DataFrame):
        close = close.iloc[:, 0]
    close = pd.to_numeric(close, errors="coerce")

    out = pd.DataFrame({"Date": pd.to_datetime(df["Date"]).dt.normalize(), "Close": close})
    out = out.dropna().sort_values("Date").reset_index(drop=True)

    out["Return"] = out["Close"].pct_change()
    out["Log_Return"] = np.log(out["Close"] / out["Close"].shift(1))
    out = out.dropna().reset_index(drop=True)
    return out


# -----------------------------
# 3) FinBERT: Load Model
# -----------------------------
@st.cache_resource(show_spinner=False)
def load_finbert() -> Tuple[AutoTokenizer, AutoModelForSequenceClassification, int, int]:
    tokenizer = AutoTokenizer.from_pretrained("ProsusAI/finbert")
    model = AutoModelForSequenceClassification.from_pretrained("ProsusAI/finbert")
    model.eval()

    id2label = {int(k): v for k, v in model.config.id2label.items()} if hasattr(model.config, "id2label") else {}

    def find_idx(target: str) -> int:
        for k, v in id2label.items():
            if target in v.lower():
                return k
        # fallback typical order
        if target == "positive":
            return 0
        if target == "negative":
            return 1
        return 2

    pos_idx = find_idx("positive")
    neg_idx = find_idx("negative")
    return tokenizer, model, pos_idx, neg_idx


def _pick_device() -> torch.device:
    # å…è®¸è‡ªåŠ¨ç”¨ GPUï¼ˆå¦‚æœéƒ¨ç½²ç¯å¢ƒæœ‰ CUDAï¼‰
    if torch.cuda.is_available():
        return torch.device("cuda")
    return torch.device("cpu")


def finbert_scores_sliding_window(
    texts: List[str],
    tokenizer,
    model,
    pos_idx: int,
    neg_idx: int,
    batch_size: int = 32,
    max_length: int = 512,
    stride: int = 128,
    mode: str = "auto",
    pooling: str = "mean",
    progress_cb=None,  # progress_cb(done_chunks, total_chunks)
) -> np.ndarray:
    """
    é•¿æ–‡æœ¬æ”¯æŒï¼š
      - mode=off: ç›´æ¥æˆªæ–­ max_length
      - mode=auto: åªæœ‰è¶…é•¿æ–‡æœ¬æ‰å¯ç”¨ overflow sliding window
      - mode=always: æ‰€æœ‰æ–‡æœ¬éƒ½å¯ç”¨ sliding window
    èšåˆï¼š
      - pooling=mean: å¯¹ä¸€ä¸ªæ–‡æ¡£çš„å¤šä¸ªçª—å£åˆ†æ•°å–å¹³å‡
      - pooling=max: å–æœ€å¤§ï¼ˆæŠ“å¼ºæƒ…ç»ªç‰‡æ®µï¼‰
    """
    if len(texts) == 0:
        return np.array([], dtype=np.float32)

    device = _pick_device()
    model = model.to(device)

    # å†³å®šæ˜¯å¦å¯ç”¨ overflow
    use_overflow = mode in ["always", "auto"]

    # ä¸ºäº†å®ç° autoï¼Œæˆ‘ä»¬éœ€è¦çŸ¥é“æ¯æ¡æ–‡æœ¬æ˜¯å¦è¶…é•¿
    # è¿™é‡Œç”¨ tokenizer ä¸æˆªæ–­åœ°å…ˆåšä¸€æ¬¡é•¿åº¦ä¼°è®¡ï¼ˆåªå– input_ids é•¿åº¦ï¼‰
    # æ³¨æ„ï¼šè¿™ä¸€æ­¥ç›¸å¯¹è½»é‡ï¼Œæ¯”è·‘æ¨¡å‹ä¾¿å®œ
    lengths = []
    if mode == "auto":
        for t in texts:
            ids = tokenizer.encode(t, add_special_tokens=True, truncation=False)
            lengths.append(len(ids))
    else:
        lengths = [max_length + 1] * len(texts)  # å¼ºåˆ¶è®¤ä¸ºè¶…é•¿ï¼Œèµ° overflowï¼ˆalwaysï¼‰

    # å¯¹æ¯æ¡æ–‡æœ¬åˆ¤æ–­ï¼šæ˜¯å¦éœ€è¦åˆ‡åˆ†
    need_chunk = [(l > max_length) for l in lengths] if mode == "auto" else ([True] * len(texts) if mode == "always" else [False] * len(texts))

    # å¦‚æœ mode=offï¼šç›´æ¥æˆªæ–­æ¨ç†ï¼ˆæ‰¹é‡å¿«ï¼‰
    if mode == "off" or (mode == "auto" and not any(need_chunk)):
        scores = []
        n = len(texts)
        for i in range(0, n, batch_size):
            batch = texts[i : i + batch_size]
            enc = tokenizer(batch, return_tensors="pt", padding=True, truncation=True, max_length=max_length)
            enc = {k: v.to(device) for k, v in enc.items()}
            with torch.no_grad():
                logits = model(**enc).logits
                probs = F.softmax(logits, dim=-1).detach().cpu().numpy()
            s = probs[:, pos_idx] - probs[:, neg_idx]
            scores.extend(s.tolist())
            if progress_cb:
                progress_cb(min(i + batch_size, n), n)
        return np.array(scores, dtype=np.float32)

    # ---------- æ··åˆï¼šçŸ­æ–‡æœ¬ç›´æ¥æˆªæ–­ï¼Œé•¿æ–‡æœ¬ sliding window ----------
    # ä¸ºäº†ç®€å•å¯é ï¼šæˆ‘ä»¬æŠŠæ‰€æœ‰æ–‡æœ¬éƒ½ç”¨ overflow tokenizer ä¸€æ¬¡æ€§å±•å¼€ä¸º chunks
    # ä½†å¯¹äºä¸éœ€è¦åˆ‡åˆ†çš„æ–‡æœ¬ï¼Œæˆ‘ä»¬å¯ä»¥è®© stride=0 å¹¶ä¿æŒå•å—ï¼›è¿™é‡Œç›´æ¥ç»Ÿä¸€èµ° overflow
    enc = tokenizer(
        texts,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=max_length,
        return_overflowing_tokens=True,
        stride=stride,
    )
    input_ids = enc["input_ids"]
    attention_mask = enc["attention_mask"]
    overflow_map = enc["overflow_to_sample_mapping"]  # æ¯ä¸ªchunkå±äºå“ªä¸ªåŸå§‹æ ·æœ¬

    total_chunks = int(input_ids.shape[0])
    # å®‰å…¨é˜ˆå€¼ï¼šé¿å…æç«¯é•¿æ–‡æœ¬å¯¼è‡´ chunk æ•°çˆ†ç‚¸
    if total_chunks > 6000:
        raise RuntimeError(f"æ–‡æœ¬è¿‡é•¿å¯¼è‡´çª—å£æ•°é‡è¿‡å¤šï¼ˆ{total_chunks} chunksï¼‰ã€‚å»ºè®®å‡å°æ•°æ®é‡æˆ–ç¼©çŸ­æ–‡æœ¬/å¢å¤§ stride/é™ä½ max_lengthã€‚")

    # å¯¹ chunk æ‰¹é‡è·‘æ¨¡å‹
    chunk_scores = np.zeros((total_chunks,), dtype=np.float32)
    done = 0
    for i in range(0, total_chunks, batch_size):
        ids = input_ids[i : i + batch_size].to(device)
        msk = attention_mask[i : i + batch_size].to(device)
        with torch.no_grad():
            logits = model(input_ids=ids, attention_mask=msk).logits
            probs = F.softmax(logits, dim=-1).detach().cpu().numpy()
        s = probs[:, pos_idx] - probs[:, neg_idx]
        chunk_scores[i : i + len(s)] = s.astype(np.float32)
        done = i + len(s)
        if progress_cb:
            progress_cb(done, total_chunks)

    # èšåˆå› doc-level åˆ†æ•°
    doc_scores: List[List[float]] = [[] for _ in range(len(texts))]
    for c_idx, doc_idx in enumerate(overflow_map.tolist()):
        doc_scores[doc_idx].append(float(chunk_scores[c_idx]))

    out = np.zeros((len(texts),), dtype=np.float32)
    for i, arr in enumerate(doc_scores):
        if not arr:
            out[i] = 0.0
        else:
            if pooling == "max":
                out[i] = float(np.max(arr))
            else:
                out[i] = float(np.mean(arr))
    return out


# -----------------------------
# 3.5) Async Inference Worker (Producer-Consumer)
# -----------------------------
@dataclass
class InferenceJob:
    job_id: str
    status: str  # queued/running/done/error
    created_at: float
    progress: float
    message: str
    scores: Optional[np.ndarray] = None
    error: Optional[str] = None


class AsyncFinBERTWorker:
    """
    åå°çº¿ç¨‹ï¼šæ¶ˆè´¹é˜Ÿåˆ—é‡Œçš„æ¨ç†ä»»åŠ¡
    - è§£è€¦ ingestion/UI ä¸ æ¨ç†
    - æ”¯æŒ long text sliding window + pooling
    """

    def __init__(self, tokenizer, model, pos_idx: int, neg_idx: int):
        self.tokenizer = tokenizer
        self.model = model
        self.pos_idx = pos_idx
        self.neg_idx = neg_idx

        self.q: "queue.Queue[Tuple[str, List[str], dict]]" = queue.Queue()
        self.jobs: Dict[str, InferenceJob] = {}
        self._lock = threading.Lock()

        self._thread = threading.Thread(target=self._loop, daemon=True)
        self._thread.start()

    def submit(self, texts: List[str], params: dict) -> str:
        job_id = uuid.uuid4().hex
        with self._lock:
            self.jobs[job_id] = InferenceJob(
                job_id=job_id,
                status="queued",
                created_at=time.time(),
                progress=0.0,
                message="Queued",
            )
        self.q.put((job_id, texts, params))
        return job_id

    def get(self, job_id: str) -> Optional[InferenceJob]:
        with self._lock:
            return self.jobs.get(job_id)

    def _update(self, job_id: str, **kwargs):
        with self._lock:
            job = self.jobs.get(job_id)
            if not job:
                return
            for k, v in kwargs.items():
                setattr(job, k, v)

    def _loop(self):
        while True:
            job_id, texts, params = self.q.get()
            try:
                self._update(job_id, status="running", message="Running", progress=0.0)

                def cb(done, total):
                    p = 0.0 if total == 0 else float(done) / float(total)
                    self._update(job_id, progress=p, message=f"Running ({done}/{total})")

                scores = finbert_scores_sliding_window(
                    texts=texts,
                    tokenizer=self.tokenizer,
                    model=self.model,
                    pos_idx=self.pos_idx,
                    neg_idx=self.neg_idx,
                    batch_size=int(params["batch_size"]),
                    max_length=int(params["max_length"]),
                    stride=int(params["stride"]),
                    mode=str(params["mode"]),
                    pooling=str(params["pooling"]),
                    progress_cb=cb,
                )
                self._update(job_id, status="done", message="Done", progress=1.0, scores=scores)

            except Exception as e:
                self._update(job_id, status="error", message="Error", error=str(e), progress=0.0)

            finally:
                self.q.task_done()


def get_worker():
    """
    åœ¨ session ä¸­æŒä¹…åŒ– workerï¼ˆæ¯ä¸ªç”¨æˆ·ä¼šè¯ä¸€ä¸ªï¼‰ï¼Œé¿å…æ¯æ¬¡ rerun é‡å»ºçº¿ç¨‹ã€‚
    """
    if "finbert_worker" not in st.session_state:
        tokenizer, finbert_model, pos_idx, neg_idx = load_finbert()
        st.session_state.finbert_worker = AsyncFinBERTWorker(tokenizer, finbert_model, pos_idx, neg_idx)
    return st.session_state.finbert_worker


# -----------------------------
# 4) Alignment: News â†’ Trading Day
# -----------------------------
def build_daily_sentiment(news_df: pd.DataFrame, method: str = "mean") -> pd.DataFrame:
    if news_df.empty:
        return pd.DataFrame()
    g = news_df.groupby("NewsDate")["Sentiment"]
    daily = g.mean() if method == "mean" else g.median()
    out = daily.reset_index().rename(columns={"NewsDate": "Date", "Sentiment": "Sentiment_Factor"})
    out["Date"] = pd.to_datetime(out["Date"]).dt.normalize()
    return out.sort_values("Date").reset_index(drop=True)


def align_sentiment_to_market(daily_sent: pd.DataFrame, market: pd.DataFrame, mode: str) -> pd.DataFrame:
    if daily_sent.empty or market.empty:
        return pd.DataFrame()

    mkt = market[["Date"]].sort_values("Date").reset_index(drop=True)
    sent = daily_sent.copy().sort_values("Date").reset_index(drop=True)

    if mode.startswith("next_trading_day"):
        sent = sent.rename(columns={"Date": "NewsDate"})
        aligned = pd.merge_asof(
            sent.sort_values("NewsDate"),
            mkt.sort_values("Date"),
            left_on="NewsDate",
            right_on="Date",
            direction="forward",
            allow_exact_matches=True,
        )
        aligned = aligned.rename(columns={"Date": "TradeDate"}).dropna(subset=["TradeDate"])
        aligned = (
            aligned.groupby("TradeDate")["Sentiment_Factor"]
            .mean()
            .reset_index()
            .rename(columns={"TradeDate": "Date"})
        )
        aligned["Date"] = pd.to_datetime(aligned["Date"]).dt.normalize()
        return aligned.sort_values("Date").reset_index(drop=True)

    aligned = pd.merge(daily_sent, market[["Date"]], on="Date", how="inner")
    return aligned[["Date", "Sentiment_Factor"]].sort_values("Date").reset_index(drop=True)


# -----------------------------
# 5) Granger: multi-lag + both directions
# -----------------------------
@dataclass
class GrangerResultRow:
    lag: int
    p_sent_to_ret: float
    p_ret_to_sent: float


def run_granger_multi_lag(merged: pd.DataFrame, max_lag: int) -> pd.DataFrame:
    df = merged[["Return", "Sentiment_Factor"]].dropna().copy()
    df = df.sort_values("Date").reset_index(drop=True)

    if len(df) < (max_lag + 12):
        raise ValueError(f"æ ·æœ¬å¤ªå°‘ï¼šéœ€è¦è‡³å°‘ ~{max_lag+12} è¡Œï¼Œå½“å‰åªæœ‰ {len(df)} è¡Œã€‚è¯·æ‰©å¤§æ—¥æœŸèŒƒå›´æˆ–æä¾›æ›´å¤šæ–°é—»ã€‚")

    ts_sr = df[["Return", "Sentiment_Factor"]].to_numpy()
    res_sr = grangercausalitytests(ts_sr, maxlag=max_lag, verbose=False)

    ts_rs = df[["Sentiment_Factor", "Return"]].to_numpy()
    res_rs = grangercausalitytests(ts_rs, maxlag=max_lag, verbose=False)

    rows = []
    for lag in range(1, max_lag + 1):
        p1 = float(res_sr[lag][0]["ssr_ftest"][1])
        p2 = float(res_rs[lag][0]["ssr_ftest"][1])
        rows.append(GrangerResultRow(lag=lag, p_sent_to_ret=p1, p_ret_to_sent=p2))

    out = pd.DataFrame([r.__dict__ for r in rows])
    out = out.rename(
        columns={
            "lag": "Lag",
            "p_sent_to_ret": "P-value (Sentiment â†’ Return)",
            "p_ret_to_sent": "P-value (Return â†’ Sentiment)",
        }
    )
    return out


# -----------------------------
# 6) Backtest: timing policy + risk metrics
# -----------------------------
def max_drawdown(equity: pd.Series) -> float:
    peak = equity.cummax()
    dd = equity / peak - 1.0
    return float(dd.min())


def sharpe_ratio(daily_ret: pd.Series, rf_annual: float = 0.02) -> float:
    rf_daily = rf_annual / 252.0
    excess = daily_ret - rf_daily
    vol = excess.std()
    if vol == 0 or np.isnan(vol):
        return 0.0
    return float(excess.mean() / vol * math.sqrt(252))


def run_timing_backtest(
    merged: pd.DataFrame,
    threshold: float = 0.0,
    cost_bps: int = 0,
    rf_annual: float = 0.02,
) -> Tuple[pd.DataFrame, dict]:
    df = merged[["Date", "Close", "Return", "Sentiment_Factor"]].dropna().copy()
    df = df.sort_values("Date").reset_index(drop=True)

    df["Position"] = (df["Sentiment_Factor"].shift(1) > threshold).astype(int)
    df["Position"] = df["Position"].fillna(0).astype(int)

    df["Trade"] = df["Position"].diff().abs().fillna(0)
    tc = (cost_bps / 10000.0) * df["Trade"]

    df["Strategy_Return"] = df["Position"] * df["Return"] - tc
    df["Benchmark_Return"] = df["Return"]

    df["Equity_Strategy"] = (1.0 + df["Strategy_Return"]).cumprod()
    df["Equity_Benchmark"] = (1.0 + df["Benchmark_Return"]).cumprod()

    df["DD_Strategy"] = df["Equity_Strategy"] / df["Equity_Strategy"].cummax() - 1.0
    df["DD_Benchmark"] = df["Equity_Benchmark"] / df["Equity_Benchmark"].cummax() - 1.0

    strat_cum = float(df["Equity_Strategy"].iloc[-1] - 1.0)
    bench_cum = float(df["Equity_Benchmark"].iloc[-1] - 1.0)

    strat_sharpe = sharpe_ratio(df["Strategy_Return"], rf_annual=rf_annual)
    bench_sharpe = sharpe_ratio(df["Benchmark_Return"], rf_annual=rf_annual)

    strat_mdd = max_drawdown(df["Equity_Strategy"])
    bench_mdd = max_drawdown(df["Equity_Benchmark"])

    strat_vol = float(df["Strategy_Return"].std() * math.sqrt(252))
    bench_vol = float(df["Benchmark_Return"].std() * math.sqrt(252))

    n_trades = int(df["Trade"].sum())
    exposure = float(df["Position"].mean())

    metrics = {
        "Strategy Cumulative Return": strat_cum,
        "Benchmark Cumulative Return": bench_cum,
        "Alpha (Strategy - Benchmark)": strat_cum - bench_cum,
        "Strategy Sharpe": strat_sharpe,
        "Benchmark Sharpe": bench_sharpe,
        "Strategy Max Drawdown": strat_mdd,
        "Benchmark Max Drawdown": bench_mdd,
        "Strategy Vol (ann.)": strat_vol,
        "Benchmark Vol (ann.)": bench_vol,
        "Trades": n_trades,
        "Exposure": exposure,
        "Transaction Cost (bps)": cost_bps,
        "Sentiment Threshold": threshold,
    }
    return df, metrics


# -----------------------------
# 7) Main Run + Tabs
# -----------------------------
tabs = st.tabs(["â‘  æ•°æ®ä¸ç®¡çº¿", "â‘¡ æƒ…ç»ªå› å­", "â‘¢ Leadâ€“Lagï¼ˆGrangerï¼‰", "â‘£ ç­–ç•¥å›æµ‹", "â‘¤ å¯¼å‡º"])

if not run_btn:
    with tabs[0]:
        st.info("ğŸ‘ˆ å…ˆåœ¨å·¦ä¾§ä¸Šä¼ æ–°é—»æ•°æ®å¹¶è®¾ç½®å‚æ•°ï¼Œç„¶åç‚¹å‡» **è¿è¡Œå…¨æµç¨‹**ã€‚")
        st.markdown(
            """
**ä½ è¿™ä»½ App å°†å±•ç¤ºï¼š**
- æ–°é—»æ ‡é¢˜/æ­£æ–‡ â†’ FinBERTï¼ˆæ”¯æŒé•¿æ–‡æœ¬æ»‘çª—ï¼‰â†’ è¿ç»­æƒ…ç»ªå› å­ï¼ˆæ—¥åº¦èšåˆï¼‰
- æƒ…ç»ªå› å­ä¸æ”¶ç›Šåºåˆ—å¯¹é½ï¼ˆæ”¯æŒå‘¨æœ«æ–°é—»æ˜ å°„åˆ°ä¸‹ä¸€äº¤æ˜“æ—¥ï¼‰
- Granger å› æœæ£€éªŒï¼ˆ1..N é˜¶ï¼Œè¾“å‡º p-valuesï¼‰
- æ‹©æ—¶ç­–ç•¥å›æµ‹ï¼ˆæ˜¨æ—¥æƒ…ç»ª>é˜ˆå€¼æŒæœ‰ï¼Œå¦åˆ™ç©ºä»“ï¼‰+ é£é™©è°ƒæ•´æŒ‡æ ‡
- å¼‚æ­¥æ¨ç†ï¼šæ¨ç†åœ¨åå°è·‘ï¼ŒUI ä¸è¢«å¡ä½ï¼ˆé€‚åˆæ–°é—» spikesï¼‰
"""
        )
    st.stop()

if uploaded_file is None:
    st.error("è¯·å…ˆä¸Šä¼ æ–°é—» CSV/Excel æ–‡ä»¶ã€‚")
    st.stop()

# è¯»å…¥æ•°æ®
file_bytes = uploaded_file.getvalue()
raw_df = load_news_cached(file_bytes, uploaded_file.name)

try:
    news = _standardize_news_columns(raw_df)
except Exception as e:
    st.error(str(e))
    st.stop()

# ticker è¿‡æ»¤
if news["Ticker"].notna().any():
    if ticker in set(news["Ticker"].dropna().unique()):
        news = news[news["Ticker"] == ticker].copy()
    else:
        st.warning(f"æ–°é—»æ–‡ä»¶ä¸­æœªå‘ç° Ticker={ticker} çš„è®°å½•ï¼Œå°†å¯¹å…¨éƒ¨æ–°é—»åšæƒ…ç»ªè®¡ç®—ã€‚")

if news.empty:
    st.error("æ¸…æ´—/ç­›é€‰åæ–°é—»ä¸ºç©ºã€‚è¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶ã€‚")
    st.stop()

# æ—¥æœŸèŒƒå›´ -> å¸‚åœºæ•°æ®
min_news_date = pd.to_datetime(news["NewsDate"].min()).normalize()
max_news_date = pd.to_datetime(news["NewsDate"].max()).normalize()
start = min_news_date - pd.Timedelta(days=5)
end = max_news_date + pd.Timedelta(days=10)

with st.spinner(f"ä¸‹è½½ {ticker} å¸‚åœºæ•°æ®å¹¶è®¡ç®—æ”¶ç›Šåºåˆ—..."):
    market = get_market_data(ticker, start=start, end=end)

if market.empty:
    st.error("æ— æ³•è·å–å¸‚åœºæ•°æ®ï¼ˆYahoo Financeï¼‰ã€‚è¯·æ£€æŸ¥ ticker æˆ–ç½‘ç»œã€‚")
    st.stop()

# ========== å¼‚æ­¥æ¨ç†ï¼šæäº¤ä»»åŠ¡ ==========
# ç”¨å‚æ•°ç­¾åé¿å…é‡å¤æäº¤ï¼ˆç”¨æˆ·åå¤åˆ‡ tab æˆ– rerun æ—¶ï¼‰
job_params = dict(
    batch_size=int(finbert_batch),
    max_length=int(window_max_len),
    stride=int(window_stride),
    mode="auto" if longtext_mode.startswith("auto") else ("always" if longtext_mode.startswith("always") else "off"),
    pooling=str(pooling),
)

headlines = news["Headline"].astype(str).tolist()

# ç”¨ session_state ç®¡ç† job ç”Ÿå‘½å‘¨æœŸ
if "infer_job_id" not in st.session_state:
    st.session_state.infer_job_id = None
    st.session_state.infer_job_sig = None

job_sig = (len(headlines), ticker, tuple(sorted(job_params.items())), float(min_news_date.value), float(max_news_date.value))

if enable_async:
    worker = get_worker()
    if st.session_state.infer_job_sig != job_sig:
        # æ–°å‚æ•°/æ–°æ•°æ® => æ–° job
        st.session_state.infer_job_id = worker.submit(headlines, job_params)
        st.session_state.infer_job_sig = job_sig
else:
    # åŒæ­¥æ¨¡å¼ï¼šç›´æ¥ç®—ï¼ˆä¼šå¡ UIï¼‰
    st.session_state.infer_job_id = "__sync__"
    st.session_state.infer_job_sig = job_sig

# å–ç»“æœï¼ˆå¦‚æœ readyï¼‰
scores = None
infer_status = None
infer_msg = ""
infer_progress = 0.0
infer_error = None

if enable_async:
    job = worker.get(st.session_state.infer_job_id) if st.session_state.infer_job_id else None
    if job:
        infer_status = job.status
        infer_msg = job.message
        infer_progress = job.progress
        infer_error = job.error
        if job.status == "done":
            scores = job.scores
else:
    # åŒæ­¥è®¡ç®—
    with st.spinner("FinBERT æ¨ç†ä¸­ï¼ˆåŒæ­¥æ¨¡å¼ä¼šå¡ä½ UIï¼‰..."):
        tokenizer, finbert_model, pos_idx, neg_idx = load_finbert()
        pbar = st.progress(0.0)

        def cb(done, total):
            pbar.progress(0.0 if total == 0 else float(done) / float(total))

        scores = finbert_scores_sliding_window(
            texts=headlines,
            tokenizer=tokenizer,
            model=finbert_model,
            pos_idx=pos_idx,
            neg_idx=neg_idx,
            batch_size=int(finbert_batch),
            max_length=int(window_max_len),
            stride=int(window_stride),
            mode=job_params["mode"],
            pooling=job_params["pooling"],
            progress_cb=cb,
        )
        pbar.empty()
        infer_status = "done"
        infer_progress = 1.0

# -----------------------------
# Tab â‘ : Data & Pipeline (å¯å…ˆçœ‹ï¼Œä¸å¡)
# -----------------------------
with tabs[0]:
    st.subheader("â‘  æ•°æ®ä¸ç®¡çº¿æ¦‚è§ˆï¼ˆNLP â†’ å› å­ â†’ å¯¹é½ï¼‰")

    c1, c2, c3, c4 = st.columns(4)
    c1.metric("æ–°é—»æ¡æ•°ï¼ˆheadline/textï¼‰", f"{len(news)}")
    c2.metric("å¸‚åœºäº¤æ˜“æ—¥æ ·æœ¬", f"{len(market)}")
    c3.metric("é•¿æ–‡æœ¬æ¨¡å¼", longtext_mode.split("ï¼ˆ")[0])
    c4.metric("å¼‚æ­¥æ¨ç†", "ON" if enable_async else "OFF")

    # æ¨ç†çŠ¶æ€åŒºï¼ˆå…³é”®ï¼šUI ä¸é˜»å¡ï¼‰
    st.markdown("### âš¡ æ¨ç†çŠ¶æ€ï¼ˆåå°è¿è¡Œï¼‰" if enable_async else "### ğŸ¢ æ¨ç†çŠ¶æ€ï¼ˆåŒæ­¥è¿è¡Œï¼‰")
    if enable_async:
        st.progress(float(infer_progress))
        if infer_status in ["queued", "running"]:
            st.info(f"FinBERT æ¨ç†ä¸­ï¼š{infer_status} Â· {infer_msg}")
            colx1, colx2 = st.columns([1, 3])
            with colx1:
                if st.button("ğŸ”„ åˆ·æ–°çŠ¶æ€/ç»§ç»­ç®¡çº¿"):
                    _rerun()
            with colx2:
                st.caption("æç¤ºï¼šå¼‚æ­¥æ¨ç†ä¸ä¼šå¡ä½ç•Œé¢ã€‚ä½ å¯ä»¥å…ˆæ£€æŸ¥æ•°æ®é¢„è§ˆï¼›æ¨ç†å®Œæˆåç‚¹å‡»åˆ·æ–°è¿›å…¥åç»­åˆ†æã€‚")
        elif infer_status == "error":
            st.error(f"æ¨ç†å¤±è´¥ï¼š{infer_error}")
        elif infer_status == "done":
            st.success("âœ… æ¨ç†å®Œæˆï¼ä½ å¯ä»¥åˆ‡æ¢åˆ°å…¶ä»– Tab æŸ¥çœ‹æƒ…ç»ª/Granger/å›æµ‹ç»“æœã€‚")
    else:
        st.success("âœ… æ¨ç†å®Œæˆï¼ˆåŒæ­¥ï¼‰ã€‚")

    left, right = st.columns(2)
    with left:
        st.markdown("**æ–°é—»æ•°æ®ï¼ˆæ¸…æ´—åï¼‰**")
        st.dataframe(news[["NewsDate", "Headline"]].head(10), use_container_width=True, height=260)
    with right:
        st.markdown("**å¸‚åœºæ•°æ®ï¼ˆæ”¶ç›Šåºåˆ—ï¼‰**")
        st.dataframe(market.head(10), use_container_width=True, height=260)

    st.caption("è¯´æ˜ï¼šè¯¥ Tab åœ¨æ¨ç†æœªå®Œæˆæ—¶ä¹Ÿå¯æ­£å¸¸æŸ¥çœ‹ï¼ˆè§£è€¦ ingestion ä¸ inferenceï¼‰ã€‚")

# å¦‚æœè¿˜æ²¡æ¨ç†å®Œï¼šåç»­ tabs ç»™å ä½æç¤ºï¼ˆä¸æŠ¥é”™ï¼‰
if scores is None:
    with tabs[1]:
        st.info("â³ æƒ…ç»ªæ¨ç†å°šæœªå®Œæˆã€‚è¯·å›åˆ° â‘  Tab ç‚¹å‡»â€œåˆ·æ–°çŠ¶æ€/ç»§ç»­ç®¡çº¿â€ã€‚")
    with tabs[2]:
        st.info("â³ æƒ…ç»ªæ¨ç†å°šæœªå®Œæˆã€‚è¯·å›åˆ° â‘  Tab ç‚¹å‡»â€œåˆ·æ–°çŠ¶æ€/ç»§ç»­ç®¡çº¿â€ã€‚")
    with tabs[3]:
        st.info("â³ æƒ…ç»ªæ¨ç†å°šæœªå®Œæˆã€‚è¯·å›åˆ° â‘  Tab ç‚¹å‡»â€œåˆ·æ–°çŠ¶æ€/ç»§ç»­ç®¡çº¿â€ã€‚")
    with tabs[4]:
        st.info("â³ æƒ…ç»ªæ¨ç†å°šæœªå®Œæˆã€‚è¯·å›åˆ° â‘  Tab ç‚¹å‡»â€œåˆ·æ–°çŠ¶æ€/ç»§ç»­ç®¡çº¿â€ã€‚")
    st.stop()

# æ¨ç†å®Œæˆï¼šå†™å…¥ Sentiment
news = news.reset_index(drop=True)
news["Sentiment"] = np.array(scores, dtype=np.float32)

# Daily factor
daily_sent = build_daily_sentiment(news, method=agg_method)

# Align to trading days
aligned_sent = align_sentiment_to_market(daily_sent, market, mode=align_mode)

# Merge aligned sentiment with market returns
merged = pd.merge(market, aligned_sent, on="Date", how="inner").sort_values("Date").reset_index(drop=True)

if len(merged) < 30:
    st.warning(f"åˆå¹¶åçš„æœ‰æ•ˆæ ·æœ¬è¾ƒå°‘ï¼ˆ{len(merged)} è¡Œï¼‰ã€‚å¯èƒ½å¯¼è‡´ Granger æ£€éªŒä¸ç¨³å®šã€‚å»ºè®®æ‰©å¤§æ–°é—»æ—¥æœŸè¦†ç›–æˆ–æ¢æ›´é•¿æ—¶é—´çª—å£ã€‚")


# -----------------------------
# Tab â‘¡: Sentiment Factor Visualization
# -----------------------------
with tabs[1]:
    st.subheader("â‘¡ æƒ…ç»ªå› å­ï¼ˆFinBERTï¼‰ä¸ä»·æ ¼èµ°åŠ¿")

    # Factor distribution
    fig_hist = go.Figure()
    fig_hist.add_trace(go.Histogram(x=news["Sentiment"], nbinsx=50, name="Headline/Text Sentiment"))
    beautify_fig(fig_hist, title="Text-level Sentiment Distribution", ytitle="Count")
    st.plotly_chart(fig_hist, use_container_width=True)

    # Price + sentiment factor subplot
    fig = make_subplots(specs=[[{"secondary_y": True}]])
    fig.add_trace(
        go.Scatter(x=merged["Date"], y=merged["Close"], name=f"{ticker} Close", mode="lines", line=dict(width=2)),
        secondary_y=False,
    )
    fig.add_trace(
        go.Bar(x=merged["Date"], y=merged["Sentiment_Factor"], name="Daily Sentiment Factor", opacity=0.55),
        secondary_y=True,
    )
    fig.update_yaxes(title_text="Price", secondary_y=False)
    fig.update_yaxes(title_text="Sentiment Factor", secondary_y=True)
    beautify_fig(fig, title="Aligned Sentiment Factor vs Price")
    st.plotly_chart(fig, use_container_width=True)

    st.caption(f"é•¿æ–‡æœ¬æ¨¡å¼ï¼š{longtext_mode} Â· pooling={pooling} Â· max_length={window_max_len} Â· stride={window_stride}")


# -----------------------------
# Tab â‘¢: Leadâ€“Lag (Granger) across multiple lags
# -----------------------------
with tabs[2]:
    st.subheader("â‘¢ Leadâ€“Lag ç»“æ„æ£€éªŒï¼šGranger Causalityï¼ˆå¤šæ»åé˜¶ï¼‰")
    st.caption("åŒæ—¶æŠ¥å‘Š Sentimentâ†’Return ä¸ Returnâ†’Sentiment çš„ p-valuesï¼ˆ1..MaxLagï¼‰ã€‚")

    try:
        gr_df = run_granger_multi_lag(merged, max_lag=max_lag)
        st.dataframe(gr_df.style.format({c: "{:.4f}" for c in gr_df.columns if "P-value" in c}), use_container_width=True)

        fig_p = go.Figure()
        fig_p.add_trace(go.Scatter(x=gr_df["Lag"], y=gr_df["P-value (Sentiment â†’ Return)"], mode="lines+markers", name="Sentiment â†’ Return"))
        fig_p.add_trace(go.Scatter(x=gr_df["Lag"], y=gr_df["P-value (Return â†’ Sentiment)"], mode="lines+markers", name="Return â†’ Sentiment"))
        fig_p.add_hline(y=0.05, line_dash="dash", annotation_text="0.05", annotation_position="top left")
        beautify_fig(fig_p, title="Granger p-values across lag orders", ytitle="p-value")
        st.plotly_chart(fig_p, use_container_width=True)

        best_lag = int(gr_df.loc[gr_df["P-value (Sentiment â†’ Return)"].idxmin(), "Lag"])
        best_p = float(gr_df["P-value (Sentiment â†’ Return)"].min())
        sig_sr = (gr_df["P-value (Sentiment â†’ Return)"] < 0.05).any()

        if sig_sr:
            st.success(f"âœ… æ£€æµ‹åˆ° **Sentiment â†’ Return** åœ¨æŸäº›æ»åé˜¶ä¸Šæ˜¾è‘—ï¼ˆp<0.05ï¼‰ã€‚æœ€å° p-valueï¼šlag={best_lag}ï¼ˆp={best_p:.4f}ï¼‰ã€‚")
        else:
            st.info(f"æœªæ£€æµ‹åˆ°æ˜¾è‘—çš„ **Sentiment â†’ Return**ï¼ˆp<0.05ï¼‰ã€‚æœ€å° p-valueï¼šlag={best_lag}ï¼ˆp={best_p:.4f}ï¼‰ã€‚")

    except Exception as e:
        st.warning(f"Granger æ£€éªŒæ— æ³•æ‰§è¡Œï¼š{e}")


# -----------------------------
# Tab â‘£: Backtest Timing Policy + Risk-Adjusted Metrics
# -----------------------------
with tabs[3]:
    st.subheader("â‘£ æ‹©æ—¶ç­–ç•¥å›æµ‹ï¼ˆTiming Policyï¼‰")
    st.markdown("ç­–ç•¥è§„åˆ™ï¼š**æ˜¨æ—¥æƒ…ç»ªå› å­ > é˜ˆå€¼ â†’ ä»Šæ—¥æŒæœ‰ï¼›å¦åˆ™ç©ºä»“**ã€‚")

    bt_df, metrics = run_timing_backtest(
        merged,
        threshold=sentiment_threshold,
        cost_bps=cost_bps,
        rf_annual=rf_annual,
    )

    c1, c2, c3 = st.columns(3)
    c1.metric("ç­–ç•¥ç´¯è®¡æ”¶ç›Š", f"{metrics['Strategy Cumulative Return']*100:.2f}%", delta=f"vs åŸºå‡† {metrics['Benchmark Cumulative Return']*100:.2f}%")
    c2.metric("ç­–ç•¥ Sharpe", f"{metrics['Strategy Sharpe']:.2f}", delta=f"vs åŸºå‡† {metrics['Benchmark Sharpe']:.2f}")
    c3.metric("ç­–ç•¥æœ€å¤§å›æ’¤", f"{metrics['Strategy Max Drawdown']*100:.2f}%", delta=f"vs åŸºå‡† {metrics['Benchmark Max Drawdown']*100:.2f}%")

    c4, c5, c6 = st.columns(3)
    c4.metric("Alphaï¼ˆç­–ç•¥-åŸºå‡†ï¼‰", f"{metrics['Alpha (Strategy - Benchmark)']*100:.2f}%")
    c5.metric("äº¤æ˜“æ¬¡æ•°ï¼ˆæ¢ä»“ï¼‰", f"{metrics['Trades']}")
    c6.metric("æš´éœ²åº¦ï¼ˆæŒä»“æ¯”ä¾‹ï¼‰", f"{metrics['Exposure']*100:.1f}%")

    fig_eq = go.Figure()
    fig_eq.add_trace(go.Scatter(x=bt_df["Date"], y=bt_df["Equity_Strategy"], name="Strategy Equity", mode="lines", line=dict(width=3)))
    fig_eq.add_trace(go.Scatter(x=bt_df["Date"], y=bt_df["Equity_Benchmark"], name="Buy&Hold Equity", mode="lines", line=dict(dash="dash")))
    beautify_fig(fig_eq, title="Equity Curve: Strategy vs Benchmark", ytitle="Equity")
    st.plotly_chart(fig_eq, use_container_width=True)

    fig_dd = go.Figure()
    fig_dd.add_trace(go.Scatter(x=bt_df["Date"], y=bt_df["DD_Strategy"], name="Strategy Drawdown", mode="lines"))
    fig_dd.add_trace(go.Scatter(x=bt_df["Date"], y=bt_df["DD_Benchmark"], name="Benchmark Drawdown", mode="lines", line=dict(dash="dash")))
    beautify_fig(fig_dd, title="Drawdown: Strategy vs Benchmark", ytitle="Drawdown")
    st.plotly_chart(fig_dd, use_container_width=True)

    fig_sig = make_subplots(specs=[[{"secondary_y": True}]])
    fig_sig.add_trace(go.Scatter(x=bt_df["Date"], y=bt_df["Sentiment_Factor"], name="Sentiment Factor", mode="lines"), secondary_y=True)
    fig_sig.add_trace(go.Scatter(x=bt_df["Date"], y=bt_df["Position"], name="Position (0/1)", mode="lines", line=dict(width=2)), secondary_y=False)
    fig_sig.update_yaxes(title_text="Position", secondary_y=False)
    fig_sig.update_yaxes(title_text="Sentiment", secondary_y=True)
    beautify_fig(fig_sig, title="Signal Construction: Prior-day Sentiment â†’ Position")
    st.plotly_chart(fig_sig, use_container_width=True)

    st.markdown("**å›æµ‹æ˜ç»†ï¼ˆå‰ 30 è¡Œï¼‰**")
    st.dataframe(bt_df.head(30), use_container_width=True, height=260)


# -----------------------------
# Tab â‘¤: Export / Download
# -----------------------------
with tabs[4]:
    st.subheader("â‘¤ å¯¼å‡ºï¼ˆå¯å¤ç°å®éªŒï¼‰")
    st.caption("ä¸‹è½½å¯¹é½åçš„æ•°æ®ã€Granger ç»“æœã€å›æµ‹æ˜ç»†ï¼Œæ–¹ä¾¿ä½ åœ¨ notebook / æŠ¥å‘Šä¸­å¤ç°ä¸ç»˜å›¾ã€‚")

    merged_csv = merged.to_csv(index=False).encode("utf-8")
    st.download_button("â¬‡ï¸ ä¸‹è½½å¯¹é½åçš„æ•°æ®ï¼ˆnews-factor-market alignedï¼‰", merged_csv, file_name=f"{ticker}_aligned_data.csv", mime="text/csv")

    try:
        gr_df = run_granger_multi_lag(merged, max_lag=max_lag)
        gr_csv = gr_df.to_csv(index=False).encode("utf-8")
        st.download_button("â¬‡ï¸ ä¸‹è½½ Granger ç»“æœï¼ˆmulti-lag p-valuesï¼‰", gr_csv, file_name=f"{ticker}_granger_pvalues.csv", mime="text/csv")
    except Exception:
        st.info("Granger ç»“æœä¸å¯ç”¨ï¼ˆæ ·æœ¬ä¸è¶³æˆ–æ£€éªŒå¤±è´¥ï¼‰ã€‚")

    bt_df, metrics = run_timing_backtest(
        merged,
        threshold=sentiment_threshold,
        cost_bps=cost_bps,
        rf_annual=rf_annual,
    )
    bt_csv = bt_df.to_csv(index=False).encode("utf-8")
    st.download_button("â¬‡ï¸ ä¸‹è½½å›æµ‹æ˜ç»†ï¼ˆpositions/returns/equityï¼‰", bt_csv, file_name=f"{ticker}_backtest_detail.csv", mime="text/csv")

    metrics_df = pd.DataFrame([metrics])
    metrics_csv = metrics_df.to_csv(index=False).encode("utf-8")
    st.download_button("â¬‡ï¸ ä¸‹è½½æŒ‡æ ‡æ±‡æ€»ï¼ˆmetricsï¼‰", metrics_csv, file_name=f"{ticker}_metrics.csv", mime="text/csv")
